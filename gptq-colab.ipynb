{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# YellowRose's super secret Colab quantization script\n",
        "## 1) Download a KoboldAI compatible Repo GPTQ-for-LLaMA and setup folders"
      ],
      "metadata": {
        "id": "g7y5klUBd2D4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive/')\n",
        "import os\n",
        "if not os.path.exists(\"/content/drive/My Drive/yr-gptq-colab\"):\n",
        "    !mkdir \"/content/drive/My Drive/yr-gptq-colab\"\n",
        "%cd \"/content/drive/My Drive/yr-gptq-colab\"\n",
        "if not os.path.exists(\"/content/drive/My Drive/yr-gptq-colab/base_models\"):\n",
        "    !mkdir \"/content/drive/My Drive/yr-gptq-colab/base_models\"\n",
        "if not os.path.exists(\"/content/drive/My Drive/yr-gptq-colab/quantized_models\"):\n",
        "    !mkdir \"/content/drive/My Drive/yr-gptq-colab/quantized_models\"\n",
        "if not os.path.exists(\"/content/drive/My Drive/yr-gptq-colab/GPTQ-for-LLaMa\"):\n",
        "    !git clone --single-branch --branch latestmerge https://github.com/YellowRoseCx/GPTQ-for-LLaMa.git\n",
        "else:\n",
        "      %cd GPTQ-for-LLaMa\n",
        "      !git pull\n",
        "%cd \"/content/drive/My Drive/yr-gptq-colab/GPTQ-for-LLaMa\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "amW_8r5xSKCu",
        "outputId": "af7481be-4292-47be-adaf-00b404992962"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": []
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2) Install GPTQ Requirements"
      ],
      "metadata": {
        "id": "1DEWtsdzd_Co"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "-APK6sdpd-5f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r requirements.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nmYxwmn_SK9j",
        "outputId": "6d901aa5-6c8c-4813-8d0c-3d2d9a5be8bf"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": []
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3) Use this space to git clone a basemodel if you want"
      ],
      "metadata": {
        "id": "2Aq4DmmD04HN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "gNyiR8Hyd8r_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd \"/content/drive/My Drive/yr-gptq-colab/base_models\"\n",
        "!git lfs install\n",
        "!git clone https://huggingface.co/TehVenom/Pygmalion-7b-Merged-Safetensors\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W-ZDJxOgSDiX",
        "outputId": "d8cedda4-ca78-44d1-f54f-c4980acd8f61"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": []
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4) Select your 16 bit base model folder from your google drive \"yr-gptq-colab/base_models/\" folder"
      ],
      "metadata": {
        "id": "iE7-HS8BeKhO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "global basemodel_folder_path\n",
        "global basemodel_folder_name\n",
        "def select_folder():\n",
        "    # get the root folder ID\n",
        "    root_id = 'root'\n",
        "    \n",
        "    # get the list of folders in the root directory\n",
        "    folder_list = []\n",
        "    file_list = os.listdir('/content/drive/My Drive/yr-gptq-colab/base_models/')\n",
        "    for file in file_list:\n",
        "        if os.path.isdir('/content/drive/My Drive/yr-gptq-colab/base_models/' + file):\n",
        "            folder_list.append(file)\n",
        "    \n",
        "    # print the list of folders\n",
        "    print('Folders in your Google Drive:')\n",
        "    if len(folder_list) >= 1:\n",
        "      for i, folder in enumerate(folder_list):\n",
        "        print(f'{i+1}: {folder}')\n",
        "    else:\n",
        "      print(\"There are no base model folders available\")\n",
        "    \n",
        "    # prompt the user to select a folder\n",
        "    while True:\n",
        "        try:\n",
        "            folder_num = int(input('Enter the number of the folder you want to use: '))\n",
        "            basemodel_folder_name = folder_list[folder_num-1]\n",
        "            basemodel_folder_path = os.path.join('/content/drive/My Drive/yr-gptq-colab/base_models/', basemodel_folder_name)\n",
        "            print(f'Selected folder: {basemodel_folder_path}')\n",
        "            print(f'Selected model: {basemodel_folder_name}')\n",
        "            return basemodel_folder_path, basemodel_folder_name\n",
        "        except:\n",
        "            print('Invalid input. Please try again.')\n",
        "\n",
        "# call the function to select a folder\n",
        "basemodel_folder_path, basemodel_folder_name = select_folder()"
      ],
      "metadata": {
        "id": "dfpvxuj1dNDh",
        "outputId": "375a2480-6aa0-4493-9a29-4ea88eb174c9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": []
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5) Setup CUDA"
      ],
      "metadata": {
        "id": "VvwyWnkqeRQf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd \"/content/drive/My Drive/yr-gptq-colab/GPTQ-for-LLaMa\"\n",
        "!python ./setup_cuda.py install #NameError: name 'quant_cuda' is not defined\n",
        "\n"
      ],
      "metadata": {
        "id": "8oKFlpB6U_OX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b8db4dda-016d-4744-e224-04f03eb1834d"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": []
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5.5) Login to Huggingface with a token generated from https://huggingface.co/settings/tokens"
      ],
      "metadata": {
        "id": "teYgrdCl-iRN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!huggingface-cli login"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JzX06b6P-mNc",
        "outputId": "cef747da-7772-4bb3-bd51-4202680bf340"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": []
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6) Quantize Model(s)\n"
      ],
      "metadata": {
        "id": "mv5RvkkNeVSj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "quant_command = (f\"CUDA_VISIBLE_DEVICES=0 python llama.py \\\"{basemodel_folder_path}\\\" c4 --wbits 4 --save_safetensors \\\"/content/drive/My Drive/yr-gptq-colab/quantized_models/{basemodel_folder_name}-4bit.safetensors\\\"\")\n",
        "print(quant_command)\n",
        "!{quant_command}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3gd7U6v4SoXa",
        "outputId": "3f68d815-9996-4e1d-801a-3cd7e6547ff4"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": []
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7) Eval Model(s)"
      ],
      "metadata": {
        "id": "8up2PEXYt7J6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "eval_command = (f\"CUDA_VISIBLE_DEVICES=0 python llama.py \\\"{basemodel_folder_path}\\\" c4 --wbits 4 --load \\\"/content/drive/My Drive/yr-gptq-colab/quantized_models/{basemodel_folder_name}-4bit.safetensors\\\" --eval --new-eval > results-{basemodel_folder_name}-eval.txt\")\n",
        "print(eval_command)\n",
        "!{eval_command}"
      ],
      "metadata": {
        "id": "lBAm5IVhuAGL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f422bb68-4d28-4d1e-bc95-3c86bed7fadd"
      },
      "execution_count": 22,
      "outputs": []
    }
  ]
}
